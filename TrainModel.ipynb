{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import dataframe_image as dfi\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Read Datasets<h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4510 entries, 0 to 4509\n",
      "Data columns (total 17 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   Unnamed: 0      4510 non-null   int64  \n",
      " 1   Text            4510 non-null   object \n",
      " 2   emoji           4510 non-null   object \n",
      " 3   Tokenized       4510 non-null   object \n",
      " 4   final_text      4494 non-null   object \n",
      " 5   text_emoji      4510 non-null   object \n",
      " 6   Pos_Word        4510 non-null   int64  \n",
      " 7   Neg_Word        4510 non-null   int64  \n",
      " 8   Total_Word      4510 non-null   int64  \n",
      " 9   Pos_Ratio       4510 non-null   float64\n",
      " 10  Neg_Ratio       4510 non-null   float64\n",
      " 11  Sentimen_Text   4510 non-null   object \n",
      " 12  Tokenize_Emoji  4510 non-null   object \n",
      " 13  Pos_Emoji       4510 non-null   int64  \n",
      " 14  Neg_Emoji       4510 non-null   int64  \n",
      " 15  Sentimen_Emoji  4510 non-null   object \n",
      " 16  Sarcasm         4510 non-null   object \n",
      "dtypes: float64(2), int64(6), object(9)\n",
      "memory usage: 599.1+ KB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Text</th>\n",
       "      <th>emoji</th>\n",
       "      <th>Tokenized</th>\n",
       "      <th>final_text</th>\n",
       "      <th>text_emoji</th>\n",
       "      <th>Pos_Word</th>\n",
       "      <th>Neg_Word</th>\n",
       "      <th>Total_Word</th>\n",
       "      <th>Pos_Ratio</th>\n",
       "      <th>Neg_Ratio</th>\n",
       "      <th>Sentimen_Text</th>\n",
       "      <th>Tokenize_Emoji</th>\n",
       "      <th>Pos_Emoji</th>\n",
       "      <th>Neg_Emoji</th>\n",
       "      <th>Sentimen_Emoji</th>\n",
       "      <th>Sarcasm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>kawan2 sunda bersatu jemput arteria dahlan</td>\n",
       "      <td>游때</td>\n",
       "      <td>['kawan', 'sunda', 'bersatu', 'jemput', 'arter...</td>\n",
       "      <td>kawan sunda bersatu jemput arteria dahlan</td>\n",
       "      <td>kawan sunda bersatu jemput arteria dahlan 游때</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Positif</td>\n",
       "      <td>['kawan', 'sunda', 'bersatu', 'jemput', 'arter...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Positif</td>\n",
       "      <td>Negatif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>arteria dahlan disidang adat sunda daerah menu...</td>\n",
       "      <td>游때</td>\n",
       "      <td>['arteria', 'dahlan', 'sidang', 'adat', 'sunda...</td>\n",
       "      <td>arteria dahlan sidang adat sunda daerah menunt...</td>\n",
       "      <td>arteria dahlan sidang adat sunda daerah menunt...</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>Positif</td>\n",
       "      <td>['arteria', 'dahlan', 'sidang', 'adat', 'sunda...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Positif</td>\n",
       "      <td>Negatif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>ga ditafsirkan membanding bandingkan edy arter...</td>\n",
       "      <td>游때</td>\n",
       "      <td>['ga', 'tafsir', 'membanding', 'membandingkan'...</td>\n",
       "      <td>ga tafsir membanding membandingkan edy arteria...</td>\n",
       "      <td>ga tafsir membanding membandingkan edy arteria...</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>Positif</td>\n",
       "      <td>['ga', 'tafsir', 'membanding', 'membandingkan'...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Positif</td>\n",
       "      <td>Negatif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>nantik ngak divhumas polri nya bilang palsu td...</td>\n",
       "      <td>游때</td>\n",
       "      <td>['nanti', 'tidak', 'divisi humas', 'polri', '....</td>\n",
       "      <td>divisi humas polri bilang palsu mengeluarkan a...</td>\n",
       "      <td>divisi humas polri bilang palsu mengeluarkan a...</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>16</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>Negatif</td>\n",
       "      <td>['divisi', 'humas', 'polri', 'bilang', 'palsu'...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Positif</td>\n",
       "      <td>Positif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>urut dada dech kuasa hukum tdk mengerti hukum ...</td>\n",
       "      <td>游때</td>\n",
       "      <td>['urut', 'dada', 'deh', 'kuasa', 'hukum', 'tid...</td>\n",
       "      <td>urut dada deh kuasa hukum mengerti hukum arter...</td>\n",
       "      <td>urut dada deh kuasa hukum mengerti hukum arter...</td>\n",
       "      <td>14</td>\n",
       "      <td>6</td>\n",
       "      <td>28</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.214286</td>\n",
       "      <td>Positif</td>\n",
       "      <td>['urut', 'dada', 'deh', 'kuasa', 'hukum', 'men...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Positif</td>\n",
       "      <td>Negatif</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                               Text  \\\n",
       "0           0         kawan2 sunda bersatu jemput arteria dahlan   \n",
       "1           2  arteria dahlan disidang adat sunda daerah menu...   \n",
       "2           4  ga ditafsirkan membanding bandingkan edy arter...   \n",
       "3           5  nantik ngak divhumas polri nya bilang palsu td...   \n",
       "4           6  urut dada dech kuasa hukum tdk mengerti hukum ...   \n",
       "\n",
       "                           emoji  \\\n",
       "0                              游때   \n",
       "1                              游때   \n",
       "2                              游때   \n",
       "3                              游때   \n",
       "4                              游때   \n",
       "\n",
       "                                           Tokenized  \\\n",
       "0  ['kawan', 'sunda', 'bersatu', 'jemput', 'arter...   \n",
       "1  ['arteria', 'dahlan', 'sidang', 'adat', 'sunda...   \n",
       "2  ['ga', 'tafsir', 'membanding', 'membandingkan'...   \n",
       "3  ['nanti', 'tidak', 'divisi humas', 'polri', '....   \n",
       "4  ['urut', 'dada', 'deh', 'kuasa', 'hukum', 'tid...   \n",
       "\n",
       "                                          final_text  \\\n",
       "0          kawan sunda bersatu jemput arteria dahlan   \n",
       "1  arteria dahlan sidang adat sunda daerah menunt...   \n",
       "2  ga tafsir membanding membandingkan edy arteria...   \n",
       "3  divisi humas polri bilang palsu mengeluarkan a...   \n",
       "4  urut dada deh kuasa hukum mengerti hukum arter...   \n",
       "\n",
       "                                          text_emoji  Pos_Word  Neg_Word  \\\n",
       "0        kawan sunda bersatu jemput arteria dahlan 游때         2         0   \n",
       "1  arteria dahlan sidang adat sunda daerah menunt...         5         4   \n",
       "2  ga tafsir membanding membandingkan edy arteria...         3         2   \n",
       "3  divisi humas polri bilang palsu mengeluarkan a...         4         8   \n",
       "4  urut dada deh kuasa hukum mengerti hukum arter...        14         6   \n",
       "\n",
       "   Total_Word  Pos_Ratio  Neg_Ratio Sentimen_Text  \\\n",
       "0           6   0.333333   0.000000       Positif   \n",
       "1           9   0.555556   0.444444       Positif   \n",
       "2           9   0.333333   0.222222       Positif   \n",
       "3          16   0.250000   0.500000       Negatif   \n",
       "4          28   0.500000   0.214286       Positif   \n",
       "\n",
       "                                      Tokenize_Emoji  Pos_Emoji  Neg_Emoji  \\\n",
       "0  ['kawan', 'sunda', 'bersatu', 'jemput', 'arter...          1          0   \n",
       "1  ['arteria', 'dahlan', 'sidang', 'adat', 'sunda...          1          0   \n",
       "2  ['ga', 'tafsir', 'membanding', 'membandingkan'...          1          0   \n",
       "3  ['divisi', 'humas', 'polri', 'bilang', 'palsu'...          1          0   \n",
       "4  ['urut', 'dada', 'deh', 'kuasa', 'hukum', 'men...          1          0   \n",
       "\n",
       "  Sentimen_Emoji  Sarcasm  \n",
       "0        Positif  Negatif  \n",
       "1        Positif  Negatif  \n",
       "2        Positif  Negatif  \n",
       "3        Positif  Positif  \n",
       "4        Positif  Negatif  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_excel('Datasets_Ready.xlsx')\n",
    "\n",
    "df.info()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove unused column/feature\n",
    "df = df.drop(columns='Unnamed: 0')\n",
    "df = df.drop(columns='Text')\n",
    "df = df.drop(columns='emoji')\n",
    "df = df.drop(columns='Tokenized')\n",
    "df = df.drop(columns='final_text')\n",
    "df = df.drop(columns='Pos_Word')\n",
    "df = df.drop(columns='Neg_Word')\n",
    "df = df.drop(columns='Total_Word')\n",
    "df = df.drop(columns='Pos_Ratio')\n",
    "df = df.drop(columns='Neg_Ratio')\n",
    "df = df.drop(columns='Tokenize_Emoji')\n",
    "df = df.drop(columns='Pos_Emoji')\n",
    "df = df.drop(columns='Neg_Emoji')\n",
    "df = df.drop(columns='Sentimen_Emoji')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_emoji</th>\n",
       "      <th>Sentimen_Text</th>\n",
       "      <th>Sarcasm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>kawan sunda bersatu jemput arteria dahlan 游때</td>\n",
       "      <td>Positif</td>\n",
       "      <td>Negatif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>arteria dahlan sidang adat sunda daerah menunt...</td>\n",
       "      <td>Positif</td>\n",
       "      <td>Negatif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ga tafsir membanding membandingkan edy arteria...</td>\n",
       "      <td>Positif</td>\n",
       "      <td>Negatif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>divisi humas polri bilang palsu mengeluarkan a...</td>\n",
       "      <td>Negatif</td>\n",
       "      <td>Positif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>urut dada deh kuasa hukum mengerti hukum arter...</td>\n",
       "      <td>Positif</td>\n",
       "      <td>Negatif</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          text_emoji Sentimen_Text  Sarcasm\n",
       "0        kawan sunda bersatu jemput arteria dahlan 游때       Positif  Negatif\n",
       "1  arteria dahlan sidang adat sunda daerah menunt...       Positif  Negatif\n",
       "2  ga tafsir membanding membandingkan edy arteria...       Positif  Negatif\n",
       "3  divisi humas polri bilang palsu mengeluarkan a...       Negatif  Positif\n",
       "4  urut dada deh kuasa hukum mengerti hukum arter...       Positif  Negatif"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Checking Max Length In Every Record</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['token'] = df['text_emoji'].apply(lambda x: x.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['max_words'] = df['token'].apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    4510.000000\n",
       "mean       11.484922\n",
       "std         6.464542\n",
       "min         1.000000\n",
       "25%         7.000000\n",
       "50%        10.000000\n",
       "75%        15.000000\n",
       "max        39.000000\n",
       "Name: max_words, dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['max_words'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['token', 'max_words'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Duplicated Data: 120\n"
     ]
    }
   ],
   "source": [
    "print(f'Total Duplicated Data: {df.duplicated().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_emoji</th>\n",
       "      <th>Sentimen_Text</th>\n",
       "      <th>Sarcasm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>acara resmi pakai bahasa indonesia tp penyelen...</td>\n",
       "      <td>Negatif</td>\n",
       "      <td>Positif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>852</th>\n",
       "      <td>contoh firaun suka menginjak harkat pribumi 游땯</td>\n",
       "      <td>Negatif</td>\n",
       "      <td>Negatif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>853</th>\n",
       "      <td>contoh firaun suka menginjak harkat pribumi 游땯</td>\n",
       "      <td>Negatif</td>\n",
       "      <td>Negatif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>859</th>\n",
       "      <td>opini cak nun kafir haman qorun suroto 游땯</td>\n",
       "      <td>Negatif</td>\n",
       "      <td>Negatif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>888</th>\n",
       "      <td>andre menilai pernyataan cak nun prabowo subia...</td>\n",
       "      <td>Negatif</td>\n",
       "      <td>Negatif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4421</th>\n",
       "      <td>drakor pencitraan mengalihkan isu 游눮</td>\n",
       "      <td>Positif</td>\n",
       "      <td>Negatif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4426</th>\n",
       "      <td>kerja pencitraan 游눮</td>\n",
       "      <td>Negatif</td>\n",
       "      <td>Positif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4505</th>\n",
       "      <td>游땴</td>\n",
       "      <td>Positif</td>\n",
       "      <td>Positif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4506</th>\n",
       "      <td>gelandangan 游땴</td>\n",
       "      <td>Negatif</td>\n",
       "      <td>Negatif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4508</th>\n",
       "      <td>bangsat 游땴</td>\n",
       "      <td>Negatif</td>\n",
       "      <td>Negatif</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>120 rows 칑 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             text_emoji Sentimen_Text  Sarcasm\n",
       "23    acara resmi pakai bahasa indonesia tp penyelen...       Negatif  Positif\n",
       "852       contoh firaun suka menginjak harkat pribumi 游땯       Negatif  Negatif\n",
       "853       contoh firaun suka menginjak harkat pribumi 游땯       Negatif  Negatif\n",
       "859            opini cak nun kafir haman qorun suroto 游땯       Negatif  Negatif\n",
       "888   andre menilai pernyataan cak nun prabowo subia...       Negatif  Negatif\n",
       "...                                                 ...           ...      ...\n",
       "4421                drakor pencitraan mengalihkan isu 游눮       Positif  Negatif\n",
       "4426                                 kerja pencitraan 游눮       Negatif  Positif\n",
       "4505                                                  游땴       Positif  Positif\n",
       "4506                                      gelandangan 游땴       Negatif  Negatif\n",
       "4508                                          bangsat 游땴       Negatif  Negatif\n",
       "\n",
       "[120 rows x 3 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[df.duplicated(), :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop_duplicates(ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_emoji</th>\n",
       "      <th>Sentimen_Text</th>\n",
       "      <th>Sarcasm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>kawan sunda bersatu jemput arteria dahlan 游때</td>\n",
       "      <td>Positif</td>\n",
       "      <td>Negatif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>arteria dahlan sidang adat sunda daerah menunt...</td>\n",
       "      <td>Positif</td>\n",
       "      <td>Negatif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ga tafsir membanding membandingkan edy arteria...</td>\n",
       "      <td>Positif</td>\n",
       "      <td>Negatif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>divisi humas polri bilang palsu mengeluarkan a...</td>\n",
       "      <td>Negatif</td>\n",
       "      <td>Positif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>urut dada deh kuasa hukum mengerti hukum arter...</td>\n",
       "      <td>Positif</td>\n",
       "      <td>Negatif</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          text_emoji Sentimen_Text  Sarcasm\n",
       "0        kawan sunda bersatu jemput arteria dahlan 游때       Positif  Negatif\n",
       "1  arteria dahlan sidang adat sunda daerah menunt...       Positif  Negatif\n",
       "2  ga tafsir membanding membandingkan edy arteria...       Positif  Negatif\n",
       "3  divisi humas polri bilang palsu mengeluarkan a...       Negatif  Positif\n",
       "4  urut dada deh kuasa hukum mengerti hukum arter...       Positif  Negatif"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Duplicated Data After Handling: 0\n"
     ]
    }
   ],
   "source": [
    "print(f'Total Duplicated Data After Handling: {df.duplicated().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_duplicated = pd.DataFrame(df.loc[df['text_emoji'].duplicated()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_duplicated.to_excel('Duplicated_Data.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Missing Value: \n",
      "text_emoji       0\n",
      "Sentimen_Text    0\n",
      "Sarcasm          0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(f'Total Missing Value: \\n{df.isnull().sum()}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Feature Extraction Using GloVe Method</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GloVe:\n",
    "    # constructors\n",
    "    def __init__(self, corpus, embedding_size, window_size=5):\n",
    "        self.corpus = corpus\n",
    "        self.embedding_size = embedding_size\n",
    "        self.window_size = window_size\n",
    "        self.word2id = {}\n",
    "        self.id2word = {}\n",
    "        self.word2count = {}\n",
    "        self.vocab_size = 0\n",
    "        self.co_matrix = None\n",
    "        self.embedding = None\n",
    "        self.bias = None\n",
    "\n",
    "    def build_vocab_information(self):\n",
    "        for sentence in self.corpus:\n",
    "            for word in sentence.split():\n",
    "                if word not in self.word2id:\n",
    "                    self.word2id[word] = self.vocab_size\n",
    "                    self.id2word[self.vocab_size] = word\n",
    "                    self.vocab_size += 1\n",
    "                self.word2count[word] = self.word2count.get(word, 0) + 1\n",
    "        print(f'Vocab Size: {self.vocab_size}')\n",
    "\n",
    "    def build_co_matrix(self):\n",
    "        self.co_matrix = np.zeros((self.vocab_size, self.vocab_size))\n",
    "        for sentence in self.corpus:\n",
    "            sentence = sentence.split()\n",
    "            for i, center_word in enumerate(sentence):\n",
    "                for j in range(max(0, i - self.window_size), i):\n",
    "                    context_word = sentence[j]\n",
    "                    self.co_matrix[self.word2id[center_word]][self.word2id[context_word]] += 1\n",
    "                    self.co_matrix[self.word2id[context_word]][self.word2id[center_word]] += 1\n",
    "\n",
    "    def train(self, num_epochs=100, learning_rate=0.01):\n",
    "        self.embedding = (np.random.rand(self.vocab_size, self.embedding_size) - 0.5) / self.embedding_size\n",
    "        self.bias = np.zeros(self.vocab_size)\n",
    "        count = 0\n",
    "        for epoch in range(num_epochs):\n",
    "            count += 1\n",
    "            loss = 0\n",
    "            for i in range(self.vocab_size):\n",
    "                for j in range(self.vocab_size):\n",
    "                    if self.co_matrix[i][j] != 0:\n",
    "                        diff = (self.embedding[i] @ self.embedding[j]) + self.bias[i] + self.bias[j] - np.log(self.co_matrix[i][j])\n",
    "                        \n",
    "                        loss += diff ** 2\n",
    "                        \n",
    "                        grad_emb_i = diff * self.embedding[j]\n",
    "                        grad_emb_j = diff * self.embedding[i]\n",
    "                        \n",
    "                        grad_bias_i = diff\n",
    "                        grad_bias_j = diff\n",
    "\n",
    "                        self.embedding[i] -= learning_rate * grad_emb_i\n",
    "                        self.embedding[j] -= learning_rate * grad_emb_j\n",
    "                        self.bias[i] -= learning_rate * grad_bias_i\n",
    "                        self.bias[j] -= learning_rate * grad_bias_j\n",
    "                        \n",
    "            print(f'Difference Iterasi {count}: {diff}\\n')\n",
    "            print(f'Loss Iterasi {count}: {loss}\\n')\n",
    "            print(f'Gradient Embedding \"i\" Iterasi {count}: {grad_emb_i}\\n')\n",
    "            print(f'Gradient Embedding \"j\" Iterasi {count}: {grad_emb_j}\\n')\n",
    "            print(f'Embedding Iterasi {count}: {self.embedding}')\n",
    "\n",
    "            print(f'Epoch {epoch + 1}/{num_epochs} - loss {round(loss, 3)}')\n",
    "            \n",
    "            if loss <= 0.001:\n",
    "                print('Training Stopped, loss less than equal 0.001')\n",
    "                break\n",
    "\n",
    "    def get_all_embeddings(self):\n",
    "        all_embeddings = {}\n",
    "        for word, word_id in self.word2id.items():\n",
    "            all_embeddings[word] = self.embedding[word_id]\n",
    "        return all_embeddings\n",
    "    \n",
    "    def get_embeddings_by_id(self, id):\n",
    "        return self.embedding[id]\n",
    "    \n",
    "    def get_embeddings(self, kata):\n",
    "        word_embeddings = {}\n",
    "        for word, word_id in self.word2id.items():\n",
    "            if word == kata:\n",
    "                word_embeddings[word] = self.embedding[word_id]\n",
    "        return word_embeddings\n",
    "\n",
    "    def embedding2word(self):\n",
    "        embedding_to_word = {}\n",
    "        for word, word_id in self.word2id.items():\n",
    "            embedding_to_word[tuple(self.embedding[word_id])] = word\n",
    "        return embedding_to_word\n",
    "    \n",
    "    def get_glove_embedding(self, sentence):\n",
    "        sentence = sentence.split()\n",
    "        embedding_vectors = []\n",
    "        for word in sentence:\n",
    "            if word in self.word2id:\n",
    "                word_id = self.word2id[word]\n",
    "                word_embedding = self.embedding[word_id]\n",
    "                embedding_vectors.append(word_embedding)\n",
    "        return embedding_vectors\n",
    "    \n",
    "    def show_comatrix(self):\n",
    "        # df_co_matrix = pd.DataFrame(self.co_matrix, index=self.word2id.keys(), columns=self.word2id.keys())\n",
    "        \n",
    "        return self.co_matrix\n",
    "    \n",
    "    def generate_coomatrix_image(self):\n",
    "        df_co_matrix = pd.DataFrame(self.co_matrix, index=self.word2id.keys(), columns=self.word2id.keys())\n",
    "\n",
    "        print('Image is being processed...')\n",
    "        dfi.export(df_co_matrix.head(10), \"coocurrenceMatrixBab3.png\", max_cols=10)\n",
    "        \n",
    "        return print('Coocurrence matrix has been successfully generated into an image')\n",
    "    \n",
    "    def show_word2id(self):\n",
    "        return self.word2id\n",
    "    \n",
    "    def show_id2word(self):\n",
    "        return self.id2word\n",
    "    \n",
    "    def show_word2count(self):\n",
    "        return self.word2count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Membangun model GloVe\n",
    "embedding_size = 50\n",
    "\n",
    "glove_model = GloVe(df['text_emoji'], embedding_size)\n",
    "\n",
    "# Membangun vocabulary\n",
    "glove_model.build_vocab_information()\n",
    "\n",
    "# Membangun co-occurrence matrix\n",
    "glove_model.build_co_matrix()\n",
    "\n",
    "# Melatih model GloVe\n",
    "glove_model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Menampilkan Dict word2id\n",
    "word_2_id = glove_model.show_word2id()\n",
    "word_2_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Menampilkan Dict id2word\n",
    "id_2_word = glove_model.show_id2word()\n",
    "id_2_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Menampilkan Dict word2count\n",
    "word_2_count = glove_model.show_word2count()\n",
    "word_2_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Menampilkan hasil dari Cooccurence Matrix\n",
    "co_matrix = glove_model.show_comatrix()\n",
    "\n",
    "co_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Generate image Matrix Coocurrence\n",
    "# generateCooMatrix = glove_model.generate_coomatrix_image()\n",
    "\n",
    "# generateCooMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embeddings = glove_model.get_embeddings('游때')\n",
    "\n",
    "print(word_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_by_id = glove_model.get_embeddings_by_id(6)\n",
    "\n",
    "embedding_by_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_embedding = glove_model.get_all_embeddings()\n",
    "\n",
    "all_embedding['kawan']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Menyimpan embedding ke file teks\n",
    "\n",
    "# embeddings = glove_model.get_all_embeddings()\n",
    "\n",
    "# output_file = \"Misc/Embeddings/embeddings50D.txt\"\n",
    "# with open(output_file, \"w\", encoding=\"utf-8\") as file:\n",
    "#     for word, embedding in embeddings.items():\n",
    "#         embedding_line = \"{} {}\\n\".format(word, \" \".join(str(value) for value in embedding))\n",
    "#         file.write(embedding_line)\n",
    "\n",
    "# print(\"Embedding telah berhasil disimpan ke: {}\".format(output_file))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Create New Feature for Word Embeddings</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_ready['embedding'] = df_ready['text_emoji'].apply(lambda x: glove_model.get_glove_embedding(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_ready.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Fungsi untuk merubah token menjadi representasi word embedding</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_token_with_word_embedding(text):\n",
    "    embeddings = []\n",
    "    for word in text.split():\n",
    "        if word in all_embedding:\n",
    "            embeddings.append(all_embedding[word])\n",
    "    \n",
    "    return np.array(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kalimat = df['text_emoji'][0]\n",
    "output = []\n",
    "\n",
    "for sentence in kalimat.split():\n",
    "    if sentence in all_embedding:\n",
    "        output.append(all_embedding[sentence])\n",
    "    \n",
    "print(len(np.array(output)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['vector'] = df['text_emoji'].apply(replace_token_with_word_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_ready['text_emoji'].duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_ready.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_ready['text_emoji'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding2word = glove_model.embedding2word()\n",
    "\n",
    "# embedding2word[tuple(df_ready['text_emoji'][0][0])]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Implement SMOTE</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Sentimen_Text'] = df['Sentimen_Text'].replace('Positif', 1)\n",
    "df['Sentimen_Text'] = df['Sentimen_Text'].replace('Negatif', 0)\n",
    "\n",
    "df['Sarcasm'] = df['Sarcasm'].replace('Positif', 1)\n",
    "df['Sarcasm'] = df['Sarcasm'].replace('Negatif', 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(df['text_emoji'])\n",
    "y = np.array(df['Sarcasm'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cek class minoritas\n",
    "print(f'Total Sample for each class:\\n{y_train.value_counts()}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Terlihat Bahwa class minoritas adalah class dengan nilai 1(Positif)</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where(y_train == 1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[np.where(y_train == 1)[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 39\n",
    "len_voc = 40000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_min = X_train[np.where(y_train == 1)[0]]\n",
    "df_train_maj = X_train[np.where(y_train == 0)[0]]\n",
    "\n",
    "df_test = X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_arrays_train_min = []\n",
    "\n",
    "for i in df_train_min:\n",
    "    new_array = np.delete(i, -1)\n",
    "    new_arrays_train_min.append(new_array)\n",
    "\n",
    "df_train_min_text = np.concatenate(new_arrays_train_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_min_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_arrays_train_maj = []\n",
    "\n",
    "for i in df_train_maj:\n",
    "    new_array = np.delete(i, -1)\n",
    "    new_arrays_train_maj.append(new_array)\n",
    "\n",
    "df_train_maj_text = np.concatenate(new_arrays_train_maj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_arrays_test = []\n",
    "\n",
    "for i in df_test:\n",
    "    new_array = np.delete(i, -1)\n",
    "    new_arrays_test.append(new_array)\n",
    "\n",
    "df_test_text = np.concatenate(new_arrays_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_train_min_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_train_maj_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_ready_min = pd.concat([df_train_min, df_test_min])\n",
    "# df_ready_maj = pd.concat([df_train_maj, df_test_maj])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizing\n",
    "def make_tokenizer(texts, len_voc):\n",
    "    from keras.preprocessing.text import Tokenizer\n",
    "    t = Tokenizer(num_words=len_voc)\n",
    "    t.fit_on_texts(texts)\n",
    "    \n",
    "    return t"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Tokenizer</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = make_tokenizer(X_train, len_voc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_test = make_tokenizer(X_test, len_voc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_train_min = make_tokenizer(df_train_min_text, len_voc)\n",
    "tokenizer_train_maj = make_tokenizer(df_train_maj_text, len_voc)\n",
    "tokenizer_test = make_tokenizer(df_test_text, len_voc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_train_min.word_index"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Sequence</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_Sequence = tokenizer.texts_to_sequences(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_Sequence = tokenizer_test.texts_to_sequences(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_min = tokenizer_train_min.texts_to_sequences(df_train_min_text)\n",
    "X_train_major = tokenizer_train_maj.texts_to_sequences(df_train_maj_text)\n",
    "X_test = tokenizer_test.texts_to_sequences(df_test_text)\n",
    "\n",
    "# X_train_Sentiment = df_train['Sentimen_Text']\n",
    "# X_test_Sentiment = df_test['Sentimen_Text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_major"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Padding</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import pad_sequences\n",
    "\n",
    "X_train_Padded = pad_sequences(X_train_Sequence, maxlen=max_len, padding='post', truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_Padded = pad_sequences(X_test_Sequence, maxlen=max_len, padding='post', truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import pad_sequences\n",
    "\n",
    "X_train_min = pad_sequences(X_train_min, maxlen=max_len, padding='post', truncating='post')\n",
    "X_train_major = pad_sequences(X_train_major, maxlen=max_len, padding='post', truncating='post')\n",
    "X_test = pad_sequences(X_test, maxlen=max_len, padding='post', truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_min"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Stacking Label Minority</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_min = y_train[np.where(y_train == 1)[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_min.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Stacking Label Majority</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_maj = y_train[np.where(y_train == 0)[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_maj.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Stacking Sentiment Minority</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_sentiment_min = []\n",
    "for i in range(len(df_train_min)):\n",
    "    X_train_sentiment_min.append(df_train_min[i][1])\n",
    "\n",
    "X_train_sentiment_min = np.array(X_train_sentiment_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_sentiment_min = X_train_sentiment_min.reshape(X_train_sentiment_min.shape[0], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_sentiment_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_sentiment_min.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Stacking Sentiment Majority</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_sentiment_maj = []\n",
    "for i in range(len(df_train_maj)):\n",
    "    X_train_sentiment_maj.append(df_train_maj[i][1])\n",
    "\n",
    "X_train_sentiment_maj = np.array(X_train_sentiment_maj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_sentiment_maj = X_train_sentiment_maj.reshape(X_train_sentiment_maj.shape[0], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_sentiment_maj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_sentiment_maj.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Stacking Sentiment Testing</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_sentiment = []\n",
    "for i in range(len(df_test)):\n",
    "    X_test_sentiment.append(df_test[i][1])\n",
    "\n",
    "X_test_sentiment = np.array(X_test_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_sentiment = X_test_sentiment.reshape(X_test_sentiment.shape[0], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_sentiment.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coefs(word,*arr): \n",
    "    return word, np.asarray(arr, dtype='float32')\n",
    "\n",
    "def load_embedding(file):\n",
    "    embeddings_index = dict(get_coefs(*i.split(\" \")) for i in open(file, encoding='utf-8'))\n",
    "    \n",
    "    return embeddings_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_embedding_matrix(embedding, tokenizer, len_voc):\n",
    "    all_embs = np.stack(embedding.values())\n",
    "    emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
    "    embed_size = all_embs.shape[1]\n",
    "    word_index = tokenizer.word_index\n",
    "    embedding_matrix = np.random.normal(emb_mean, emb_std, (len_voc, embed_size))\n",
    "    # embedding_matrix = np.zeros((len_voc, embed_size))\n",
    "    \n",
    "    for word, i in word_index.items():\n",
    "        if i >= len_voc:\n",
    "            continue\n",
    "        embedding_vector = embedding.get(word)\n",
    "        if embedding_vector is not None: \n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    \n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove = load_embedding('Misc/Embeddings/embeddings100D.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.1664963 , -0.1963187 , -0.1384915 , -0.11371673,  0.0414871 ,\n",
       "       -0.11432921,  0.10378131, -0.03174866,  0.05559847,  0.24566665,\n",
       "        0.04883485,  0.16087544,  0.04765552,  0.0860668 , -0.1036344 ,\n",
       "       -0.10560928, -0.05109665, -0.01595475, -0.11956628,  0.1289334 ,\n",
       "       -0.19900735, -0.04745169, -0.05132945,  0.0320598 ,  0.03187682,\n",
       "       -0.07786787, -0.14429773,  0.23841153, -0.26779142,  0.09531006,\n",
       "       -0.12839785,  0.0138438 ,  0.05771478, -0.06913733,  0.2861561 ,\n",
       "        0.08038941,  0.0523537 ,  0.07629201,  0.05946291,  0.1258838 ,\n",
       "       -0.00635946,  0.0659649 , -0.00871249,  0.01503011, -0.02720634,\n",
       "        0.1058095 , -0.16123803, -0.15095785, -0.3382308 , -0.19761616,\n",
       "       -0.23972012,  0.16163072,  0.01586286,  0.07370322, -0.17882337,\n",
       "        0.02930516, -0.26223096,  0.23496465,  0.04109401,  0.01506277,\n",
       "       -0.05834723, -0.00463794, -0.23533121, -0.02478273, -0.00846084,\n",
       "       -0.20255427,  0.02095671,  0.22667049, -0.19158429, -0.06915851,\n",
       "        0.02082709, -0.10669942, -0.01668901, -0.04685685, -0.15410654,\n",
       "       -0.02811927, -0.17955963, -0.04021009, -0.14443895,  0.05836901,\n",
       "       -0.08712931, -0.1977816 ,  0.00178699,  0.10110352,  0.14875847,\n",
       "       -0.00523498,  0.00567863,  0.10589311,  0.19639044, -0.00256115,\n",
       "        0.05668325, -0.21922936, -0.08868878,  0.21383485, -0.11844841,\n",
       "        0.03084472, -0.24673432, -0.05325164, -0.01233548,  0.12542267],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove.get('foto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\AppData\\Roaming\\Python\\Python39\\site-packages\\IPython\\core\\interactiveshell.py:3378: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "embed_matrix = make_embedding_matrix(glove, tokenizer, len_voc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_embedding = embed_matrix[X_train_Padded]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_matrix_test = make_embedding_matrix(glove, tokenizer_test, len_voc)\n",
    "\n",
    "X_test_embedding = embed_matrix_test[X_test_Padded]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size, max_len, embed_size = X_train_embedding.shape\n",
    "X_train_embedding_r = X_train_embedding.reshape(train_size, max_len*embed_size)\n",
    "\n",
    "test_size, max_len, embed_size = X_test_embedding.shape\n",
    "X_test_embedding_r = X_test_embedding.reshape(test_size, max_len*embed_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "smote = SMOTE()\n",
    "\n",
    "X_oversampled, y_oversampled = smote.fit_resample(X_train_embedding_r, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 2012\n",
      "1: 2012\n"
     ]
    }
   ],
   "source": [
    "unique_values, value_counts = np.unique(y_oversampled, return_counts=True)\n",
    "\n",
    "for value, counts in zip(unique_values, value_counts):\n",
    "    print(f'{value}: {counts}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_mat_train_min = make_embedding_matrix(glove, tokenizer_train_min, len_voc)\n",
    "embed_mat_train_maj = make_embedding_matrix(glove, tokenizer_train_maj, len_voc)\n",
    "\n",
    "embed_mat_test = make_embedding_matrix(glove, tokenizer_test, len_voc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_mat_train_min.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_emb_minority = embed_mat_train_min[X_train_min]\n",
    "X_train_emb_majority = embed_mat_train_maj[X_train_major]\n",
    "\n",
    "X_test_emb = embed_mat_test[X_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_emb_minority"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size_min, max_len_min, embed_size_min = X_train_emb_minority.shape\n",
    "X_train_emb_r_min = X_train_emb_minority.reshape(train_size_min, max_len*embed_size_min)\n",
    "\n",
    "train_size_maj, max_len_maj, embed_size_maj = X_train_emb_majority.shape\n",
    "X_train_emb_r_maj = X_train_emb_majority.reshape(train_size_maj, max_len*embed_size_maj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_emb_r_min.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_emb_r_maj.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size, max_len, embed_size = X_test_emb.shape\n",
    "X_test_emb_r = X_test_emb.reshape(test_size, max_len*embed_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_emb_r.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Stacking All Data Minority & Majority</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stack_train_minority = np.hstack((X_train_emb_r_min, X_train_sentiment_min))\n",
    "stack_train_majority = np.hstack((X_train_emb_r_maj, X_train_sentiment_maj))\n",
    "\n",
    "stack_testing = np.hstack((X_test_emb_r, X_test_sentiment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Shape stack minority: {stack_train_minority.shape}')\n",
    "print(f'Shape stack majority: {stack_train_majority.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_size, max_len, embed_size = X_test_emb.shape\n",
    "# X_test_emb_r = X_test_emb.reshape(test_size, max_len*embed_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = df_ready['embedding']\n",
    "# X_sentiment = np.array(df_ready['Sentimen_Text']).reshape(-1, 1)\n",
    "# y = np.array(df_ready['Sarcasm'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "# # Bagi dataset menjadi train set dan test set\n",
    "# X_train, X_test, X_sentiment_train, X_sentiment_test, y_train, y_test = train_test_split(X, X_sentiment, y, test_size=0.4, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_minority = X_train[y_train == 1]\n",
    "# X_sentiment_minority = X_sentiment_train[y_train == 1]\n",
    "# y_minority = y_train[y_train == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(X_minority)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "\n",
    "def euclidean_distance(X1, X2):\n",
    "    return euclidean_distances(X1, X2)\n",
    "\n",
    "def get_neighbors(distances, k):\n",
    "    return np.argsort(distances)[:, 1:k+1]\n",
    "\n",
    "def generate_synthetic_samples(X_minority, neighbors, k_neighbors):\n",
    "    synthetic_samples = []\n",
    "\n",
    "    for i in range(len(X_minority)):\n",
    "        minority_neighbors = neighbors[i][neighbors[i] < len(X_minority)]\n",
    "        nn = np.random.choice(minority_neighbors, size=min(k_neighbors, len(minority_neighbors)), replace=False)\n",
    "        alpha = np.random.uniform(0, 1)\n",
    "        synthetic_sample = X_minority[i] + alpha * (X_minority[nn] - X_minority[i])\n",
    "        \n",
    "        if synthetic_sample.size > 1:\n",
    "            synthetic_sample = synthetic_sample[:1]\n",
    "\n",
    "        print(synthetic_sample)\n",
    "        \n",
    "        synthetic_samples.append(synthetic_sample)\n",
    "\n",
    "    data_synthetic = np.vstack(synthetic_samples)\n",
    "    for i in range(len(synthetic_samples)):\n",
    "        print(synthetic_samples[i].shape)\n",
    "    return data_synthetic\n",
    "\n",
    "def smote(X_minority, X_majority, y_minority, k):\n",
    "    minority_size = X_minority.shape[0]\n",
    "    majority_size = X_majority.shape[0]  # Asumsi jumlah sampel mayoritas sama dengan jumlah sampel minoritas\n",
    "    \n",
    "    # Menghitung jumlah tetangga terdekat berdasarkan perbandingan antara jumlah sampel minoritas dan mayoritas\n",
    "    k_neighbors = int((majority_size / minority_size) * k)\n",
    "    \n",
    "    if k_neighbors > k:\n",
    "        k_neighbors = k\n",
    "\n",
    "    # Menghitung jarak antara sampel minoritas dan mayoritas\n",
    "    distances = euclidean_distance(X_minority, X_majority)\n",
    "    print('Distance\\n')\n",
    "    print(f'{distances}\\n')\n",
    "    \n",
    "    # Mendapatkan tetangga terdekat untuk setiap sampel minoritas\n",
    "    neighbors = get_neighbors(distances, k_neighbors)\n",
    "    print('Neighbors\\n')\n",
    "    print(f'{neighbors}\\n')\n",
    "    print(f'Length : {neighbors.shape[0]}\\n')\n",
    "    \n",
    "    # Menghasilkan sampel sintetis\n",
    "    print('Synthetic Data\\n')\n",
    "    synthetic_samples = generate_synthetic_samples(X_minority, neighbors, k_neighbors)\n",
    "    synthetic_labels = np.ones(len(synthetic_samples))\n",
    "    \n",
    "    # Menggabungkan sampel minoritas asli dengan sampel sintetis\n",
    "    X_oversampled = np.vstack((X_minority, synthetic_samples))\n",
    "    y_oversampled = np.hstack((y_minority, synthetic_labels))\n",
    "    \n",
    "    return X_oversampled, y_oversampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_size = train_size_maj + train_size_min + test_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Menjalankan SMOTE pada data stack\n",
    "X_minority = stack_train_minority\n",
    "y_minority = y_train_min\n",
    "X_majority = stack_train_minority\n",
    "y_majority = y_train_maj\n",
    "\n",
    "X_oversampled, y_oversampled = smote(X_minority, X_majority, y_minority, k=5)\n",
    "\n",
    "# Memisahkan kembali data menjadi train dan test set\n",
    "X_train_oversampled = X_oversampled[:total_size]\n",
    "X_test_oversampled = X_oversampled[total_size:]\n",
    "\n",
    "y_train_oversampled = y_oversampled[:total_size]\n",
    "y_test_oversampled = y_oversampled[total_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_stack = np.vstack((stack_train_majority, X_oversampled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(final_stack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_stack = np.delete(final_stack, np.s_[4024::], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_stack.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_stack_label = np.hstack((y_train_maj, y_oversampled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_stack_label = np.delete(final_stack_label, np.s_[4024::], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Final Shape Data: {final_stack.shape}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Final Shape Label: {final_stack_label.shape}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Menghitung Jumlah Akhir Sample pada Class 0 dan 1\n",
    "unique, counts = np.unique(final_stack_label, return_counts=True)\n",
    "result = np.asarray((unique, counts)).T.astype(int)\n",
    "\n",
    "print(result[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = ['Sebelum', 'Sesudah']\n",
    "y = [len(X_train_emb_r_min) + len(X_train_emb_r_maj), len(final_stack)]\n",
    "\n",
    "\n",
    "plt.bar(x, y, color=[(0.38, 0.62, 0.79), (0.98, 0.64, 0.33)], edgecolor='black', linewidth=1.5)\n",
    "\n",
    "for i in range(len(x)):\n",
    "    plt.text(x[i], y[i], str(y[i]), ha='center', va='bottom')\n",
    "\n",
    "plt.ylabel('Jumlah Sample')\n",
    "plt.title('Perbandingan Data Training Setelah Diimplementasi SMOTE')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = ['0', '1']\n",
    "y = [1794, 1718]\n",
    "\n",
    "\n",
    "plt.bar(x, y, color=[(0.38, 0.62, 0.79), (0.98, 0.64, 0.33)], edgecolor='black', linewidth=1.5)\n",
    "\n",
    "for i in range(len(x)):\n",
    "    plt.text(x[i], y[i], str(y[i]), ha='center', va='bottom')\n",
    "\n",
    "plt.ylabel('Jumlah Sample')\n",
    "plt.title('Jumlah Sample Masing-Masing Kelas Pada Data Training Sebelum Di Implementasi SMOTE')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = ['0', '1']\n",
    "y = [result[0][1], result[1][1]]\n",
    "\n",
    "\n",
    "plt.bar(x, y, color=[(0.38, 0.62, 0.79), (0.98, 0.64, 0.33)], edgecolor='black', linewidth=1.5)\n",
    "\n",
    "for i in range(len(x)):\n",
    "    plt.text(x[i], y[i], str(y[i]), ha='center', va='bottom')\n",
    "\n",
    "plt.ylabel('Jumlah Sample')\n",
    "plt.title('Jumlah Sample Masing-Masing Kelas Setelah Di Implementasi SMOTE')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_embedding = np.array(df_ready['embedding'][:2]).reshape(-1, 1)\n",
    "# sentiment_values = np.array(df_ready['Sentimen_Text'][:2]).reshape(-1, 1)\n",
    "\n",
    "# num_rows = X_embedding.shape[0]\n",
    "\n",
    "# X_sentiment = np.empty((num_rows, 1), dtype=np.str_)\n",
    "\n",
    "# for i in range(num_rows):\n",
    "#     X_sentiment[i] = sentiment_values[i % sentiment_values.shape[0]]\n",
    "\n",
    "# X = np.hstack((X_embedding, X_sentiment))\n",
    "\n",
    "# X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y = df_ready['Sarcasm']\n",
    "\n",
    "# y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# def get_k_nearest_neighbors(X, sample, k):\n",
    "#     # Mencari tetangga terdekat menggunakan k-NN\n",
    "#     knn = NearestNeighbors(n_neighbors=k+1)\n",
    "#     knn.fit(X)\n",
    "    \n",
    "#     # Mengembalikan indeks tetangga terdekat untuk sampel\n",
    "#     _, indices = knn.kneighbors([sample])\n",
    "#     indices_1d = indices.flatten()  # Mengubah menjadi 1D array\n",
    "    \n",
    "#     return indices_1d[1:]  # Menghilangkan indeks diri sendiri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import math\n",
    "\n",
    "# def SMOTE(df, k, oversampling_ratio):\n",
    "#     # Mendapatkan array numpy dari fitur embedding, sentiment, dan label\n",
    "#     embeddings = np.vstack(df['embedding'].to_numpy())\n",
    "#     labels = df['Sarcasm'].to_numpy()\n",
    "    \n",
    "#     # Menginisialisasi array untuk menyimpan sampel sintetis yang dihasilkan\n",
    "#     synthetic_samples = []\n",
    "    \n",
    "#     # Mencari indeks sampel minoritas dan mayoritas\n",
    "#     minority_indices = np.where(labels == 1)[0]\n",
    "#     majority_indices = np.where(labels == 0)[0]\n",
    "    \n",
    "#     # Menghitung jumlah sampel sintetis yang akan dibuat untuk kelas minoritas\n",
    "#     num_minority_samples = len(minority_indices)\n",
    "#     num_majority_samples = len(majority_indices)\n",
    "    \n",
    "#     num_synthetic_samples = min(num_majority_samples - num_minority_samples, math.ceil(num_minority_samples * oversampling_ratio))\n",
    "    \n",
    "#     if num_synthetic_samples <= 0:\n",
    "#         return pd.DataFrame()  # Return empty DataFrame jika tidak ada sampel sintetis yang perlu dibuat\n",
    "    \n",
    "#     # Looping melalui setiap sampel minoritas\n",
    "#     for i in minority_indices:\n",
    "#         sample = embeddings[i]\n",
    "        \n",
    "#         # Mencari tetangga terdekat untuk sampel minoritas\n",
    "#         neighbors = get_k_nearest_neighbors(embeddings, sample, k)\n",
    "        \n",
    "#         # Menghasilkan sampel sintetis\n",
    "#         for j in range(num_synthetic_samples):\n",
    "#             # Memilih satu tetangga acak\n",
    "#             neighbor_index = np.random.choice(neighbors)\n",
    "            \n",
    "#             if neighbor_index >= len(embeddings):\n",
    "#                 continue  # Skip jika indeks tetangga melebihi ukuran data yang valid\n",
    "            \n",
    "#             neighbor = embeddings[neighbor_index]\n",
    "            \n",
    "#             # Menghitung selisih antara sampel dan tetangga\n",
    "#             diff = neighbor - sample\n",
    "            \n",
    "#             # Menghasilkan sampel sintetis dengan proporsi acak\n",
    "#             proportion = np.random.uniform(0, 1, size=1)\n",
    "#             synthetic_sample = sample + proportion * diff\n",
    "            \n",
    "#             # Menambahkan sampel sintetis ke array\n",
    "#             synthetic_samples.append(synthetic_sample)\n",
    "    \n",
    "#     # Mengubah array numpy ke DataFrame\n",
    "#     embedding_array = np.vstack(synthetic_samples)\n",
    "#     embedding_df = pd.DataFrame(embedding_array, columns=['embedding'] * embedding_array.shape[1])\n",
    "#     sarcasm_df = pd.DataFrame(np.zeros((embedding_array.shape[0], 1)), columns=['Sarcasm'])\n",
    "#     synthetic_df = pd.concat([embedding_df, sarcasm_df], axis=1)\n",
    "    \n",
    "#     # Menambahkan label kelas minoritas pada data sintetis\n",
    "#     synthetic_df['Sarcasm'] = 1\n",
    "\n",
    "#     return synthetic_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# oversampled_df = SMOTE(df_ready, k=5, oversampling_ratio=1.0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Training Bi-LSTM<h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bi-LSTM Algorithm\n",
    "\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "\n",
    "# activation function sigmoid\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Turunan activation function sigmoid\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "# activation function tanh\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "# Turunan activation function tanh\n",
    "def tanh_derivative(x):\n",
    "    return 1 - x**2\n",
    "\n",
    "class BiLSTM:\n",
    "    # Constructor\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        # Parameter untuk Input Gate Menggunakan Metode Xavier\n",
    "        self.W_i = np.random.uniform(-1, 1, (hidden_size, input_size)) * np.sqrt(1 / input_size)\n",
    "        self.U_i = np.random.uniform(-1, 1, (hidden_size, hidden_size)) * np.sqrt(1 / hidden_size)\n",
    "        self.b_i = np.zeros((hidden_size, 1))\n",
    "        \n",
    "        # Parameter untuk Forget Gate Menggunakan Metode Xavier\n",
    "        self.W_f = np.random.uniform(-1, 1, (hidden_size, input_size)) * np.sqrt(1 / input_size)\n",
    "        self.U_f = np.random.uniform(-1, 1, (hidden_size, hidden_size)) * np.sqrt(1 / hidden_size)\n",
    "        self.b_f = np.zeros((hidden_size, 1))\n",
    "        \n",
    "        # Parameter untuk Output Gate Menggunakan Metode Xavier\n",
    "        self.W_o = np.random.uniform(-1, 1, (hidden_size, input_size)) * np.sqrt(1 / input_size)\n",
    "        self.U_o = np.random.uniform(-1, 1, (hidden_size, hidden_size)) * np.sqrt(1 / hidden_size)\n",
    "        self.b_o = np.zeros((hidden_size, 1))\n",
    "        \n",
    "        # Parameter untuk Cell State Menggunakan Metode Xavier\n",
    "        self.W_c = np.random.uniform(-1, 1, (hidden_size, input_size)) * np.sqrt(1 / input_size)\n",
    "        self.U_c = np.random.uniform(-1, 1, (hidden_size, hidden_size)) * np.sqrt(1 / hidden_size)\n",
    "        self.b_c = np.zeros((hidden_size, 1))\n",
    "        \n",
    "        # Parameter untuk output layer Menggunakan Metode Xavier\n",
    "        self.W_y = np.random.uniform(-1, 1, (output_size, hidden_size*2)) * np.sqrt(1 / (hidden_size*2))\n",
    "        self.b_y = np.zeros((output_size, 1))\n",
    "\n",
    "        self.count = 0\n",
    "        \n",
    "    def forward_backward(self, x):\n",
    "        T = len(x)\n",
    "        # print(T)\n",
    "        self.count += 1\n",
    "\n",
    "        self.h_forward = np.zeros((T + 1, self.hidden_size))\n",
    "        self.c_forward = np.zeros((T + 1, self.hidden_size))\n",
    "        self.h_backward = np.zeros((T + 1, self.hidden_size))\n",
    "        self.c_backward = np.zeros((T + 1, self.hidden_size))\n",
    "        self.gates_forward = np.zeros((T, self.hidden_size*4, self.hidden_size))\n",
    "        self.gates_backward = np.zeros((T, self.hidden_size*4, self.hidden_size))\n",
    "        self.outputs = np.zeros((T, self.output_size))\n",
    "        \n",
    "        for t in range(T):\n",
    "            # Forward\n",
    "            self.gates_forward[t] = np.concatenate([\n",
    "                sigmoid(self.W_i @ x[t] + self.U_i @ self.h_forward[t] + self.b_i),\n",
    "                sigmoid(self.W_f @ x[t] + self.U_f @ self.h_forward[t] + self.b_f),\n",
    "                sigmoid(self.W_o @ x[t] + self.U_o @ self.h_forward[t] + self.b_o),\n",
    "                tanh(self.W_c @ x[t] + self.U_c @ self.h_forward[t] + self.b_c)\n",
    "            ])\n",
    "        \n",
    "            # print(f'Forget Gate Forward Iterasi {self.count}: {sigmoid(self.W_f @ x[t] + self.U_f @ self.h_forward[t] + self.b_f)}\\n')\n",
    "            # print(f'Input Gate Forward Iterasi {self.count}: {sigmoid(self.W_i @ x[t] + self.U_i @ self.h_forward[t] + self.b_i)}\\n')\n",
    "            # print(f'Output Gate Forward Iterasi {self.count}: {sigmoid(self.W_o @ x[t] + self.U_o @ self.h_forward[t] + self.b_o)}\\n')\n",
    "            self.c_forward[t + 1] = self.gates_forward[t, 1] * self.c_forward[t] + self.gates_forward[t, 0] * self.gates_forward[t, 3]\n",
    "            self.h_forward[t + 1] = self.gates_forward[t, 2] * tanh(self.c_forward[t + 1])\n",
    "\n",
    "            # print(f'Cell State Forward Iterasi {self.count}: {self.gates_forward[t, 1] * self.c_forward[t] + self.gates_forward[t, 0] * self.gates_forward[t, 3]}\\n')\n",
    "            # print(f'Hidden State Forward Iterasi {self.count}: {self.gates_forward[t, 2] * tanh(self.c_forward[t + 1])}\\n')\n",
    "\n",
    "            # backward\n",
    "            t_backward = T - t - 1\n",
    "            self.gates_backward[t_backward] = np.concatenate([\n",
    "                sigmoid(self.W_i @ x[t_backward] + self.U_i @ self.h_backward[t_backward] + self.b_i),\n",
    "                sigmoid(self.W_f @ x[t_backward] + self.U_f @ self.h_backward[t_backward] + self.b_f),\n",
    "                sigmoid(self.W_o @ x[t_backward] + self.U_o @ self.h_backward[t_backward] + self.b_o),\n",
    "                tanh(self.W_c @ x[t_backward] + self.U_c @ self.h_backward[t_backward] + self.b_c)\n",
    "            ])\n",
    "            \n",
    "            # print(f'Forget Gate Backward Iterasi {self.count}: {sigmoid(self.W_f @ x[t_backward] + self.U_f @ self.h_backward[t_backward] + self.b_f)}\\n')\n",
    "            # print(f'Input Gate Backward Iterasi {self.count}: {sigmoid(self.W_i @ x[t_backward] + self.U_i @ self.h_backward[t_backward] + self.b_i)}\\n')\n",
    "            # print(f'Output Gate Backward Iterasi {self.count}: {sigmoid(self.W_o @ x[t_backward] + self.U_o @ self.h_backward[t_backward] + self.b_o)}\\n')\n",
    "            self.c_backward[t_backward + 1] = self.gates_backward[t_backward, 1] * self.c_backward[t_backward] + self.gates_backward[t_backward, 0] * self.gates_backward[t_backward, 3]\n",
    "            self.h_backward[t_backward + 1] = self.gates_backward[t_backward, 2] * tanh(self.c_backward[t_backward + 1])\n",
    "\n",
    "            # print(f'Cell State Backward Iterasi {self.count}: {self.gates_backward[t_backward, 1] * self.c_backward[t_backward] + self.gates_backward[t_backward, 0] * self.gates_backward[t_backward, 3]}\\n')\n",
    "            # print(f'Hidden State Backward Iterasi {self.count}: {self.gates_backward[t_backward, 2] * tanh(self.c_backward[t_backward + 1])}\\n')\n",
    "\n",
    "            # Merged Forward and Backward LSTM\n",
    "            merged_output = np.concatenate([self.h_forward[t + 1], self.h_backward[t_backward + 1]])\n",
    "            # h_reshaped = np.reshape(merged_output, (self.hidden_size*2,))\n",
    "            # print(f'Final Output Reshape: {h_reshaped}\\n')\n",
    "            sigmoids = sigmoid(self.W_y @ merged_output + self.b_y)\n",
    "            # print(f'Sigmoids: {sigmoids.shape}\\n')\n",
    "            self.outputs[t] = sigmoids[0]\n",
    "        # print(f'Merged Output: {merged_output}\\n')\n",
    "        # print(f'final Outputs{self.outputs}\\n')\n",
    "        \n",
    "        return self.outputs\n",
    "    \n",
    "    def backPropagation(self, x, y, learning_rate):\n",
    "        T = len(x)\n",
    "        dW_i, dU_i, db_i = np.zeros_like(self.W_i), np.zeros_like(self.U_i), np.zeros_like(self.b_i)\n",
    "        dW_f, dU_f, db_f = np.zeros_like(self.W_f), np.zeros_like(self.U_f), np.zeros_like(self.b_f)\n",
    "        dW_o, dU_o, db_o = np.zeros_like(self.W_o), np.zeros_like(self.U_o), np.zeros_like(self.b_o)\n",
    "        dW_c, dU_c, db_c = np.zeros_like(self.W_c), np.zeros_like(self.U_c), np.zeros_like(self.b_c)\n",
    "        dW_y, db_y = np.zeros_like(self.W_y), np.zeros_like(self.b_y)\n",
    "        dc_next = np.zeros((1, self.hidden_size))\n",
    "        dh_next = np.zeros((1, self.hidden_size))\n",
    "        \n",
    "        for t in reversed(range(T)):\n",
    "            dy = self.outputs[t]\n",
    "            # print(f'Outputs: {dy}\\n')\n",
    "            # print(f'{y[t]}\\n')\n",
    "            dy[y[t]] -= 1\n",
    "            # print(f'Outputs minus 1{dy}\\n')\n",
    "            dy_transpose = dy.reshape(1, -1).T\n",
    "            # print(f'Shape dy_transpose: {dy_transpose.shape}\\n')\n",
    "            # print(f'Shape self.W_y.T : {self.W_y.shape}\\n')\n",
    "            dh_forward = self.W_y.T @ dy_transpose + dh_next\n",
    "            # print(f'Shape dh_forward: {dh_forward.shape}\\n')\n",
    "            dc_forward = dh_forward * self.gates_forward[t, 2] * tanh_derivative(tanh(self.c_forward[t + 1])) + dc_next\n",
    "            dh_backward = self.W_y.T @ dy_transpose + dh_next\n",
    "            dc_backward = dh_backward * self.gates_backward[t, 2] * tanh_derivative(tanh(self.c_backward[t + 1])) + dc_next\n",
    "            \n",
    "            # print(f'Shape dy_transpose: {dy_transpose.shape}\\n')\n",
    "            # print(f'Shape dW_y: {dW_y.shape}\\n')\n",
    "            # print(f'Shape self.h_forward: {self.h_forward[t + 1].shape}\\n')\n",
    "            # dW_y += dy_transpose @ np.expand_dims(self.h_forward[t + 1], axis=0) + dy_transpose @ np.expand_dims(self.h_backward[t + 1], axis=0) # Salah\n",
    "            dW_y += dy_transpose @ np.expand_dims(np.concatenate([self.h_forward[t + 1], self.h_backward[t + 1][:self.hidden_size]]), axis=0) # Benar\n",
    "            db_y += dy_transpose\n",
    "            \n",
    "            # print(f'dc_forward: {dc_forward.shape}, self.gates_forward[t, 3]: {self.gates_forward[t, 3].shape}\\n')\n",
    "            dg_forward = dc_forward * self.gates_forward[t, 2] * tanh_derivative(self.gates_forward[t, 3])\n",
    "            dg_backward = dc_backward * self.gates_backward[t, 3]\n",
    "\n",
    "            dgates_forward = self.gates_forward[t]\n",
    "            # print(f'Shape dgates_forward[3]: {dgates_forward.shape}\\n')\n",
    "            # dg_forward_expanded = np.expand_dims(dg_forward, axis=0)\n",
    "            dgated = dg_forward * tanh_derivative(dgates_forward[3])\n",
    "            # print(f'tanh_derivative Shape: {dgated.shape}\\n')\n",
    "            # print(f'Shape dg_forward : {dg_forward.shape}')\n",
    "            dgates_forward[3][:self.hidden_size] = np.sum(dg_forward[:self.hidden_size] * tanh_derivative(dgates_forward[3])[:self.hidden_size], axis=1) # Error disini\n",
    "            dgates_forward[0][:self.hidden_size] = np.sum(dc_forward[:self.hidden_size] * self.gates_forward[t, 0], axis=1)\n",
    "            dgates_forward[1][:self.hidden_size] = np.sum(dc_forward[:self.hidden_size] * self.c_forward[t], axis=1)\n",
    "            dgates_forward[2][:self.hidden_size] = np.sum(dh_forward[:self.hidden_size] * tanh(self.c_forward[t + 1]) * sigmoid_derivative(dgates_forward[2]), axis=1)\n",
    "\n",
    "            dgates_backward = self.gates_backward[t]\n",
    "            dgates_backward[3][:self.hidden_size] = np.sum(dg_backward[:self.hidden_size] * tanh_derivative(dgates_backward[3]))\n",
    "            dgates_backward[0][:self.hidden_size] = np.sum(dc_backward[:self.hidden_size] * self.gates_backward[t, 0])\n",
    "            dgates_backward[1][:self.hidden_size] = np.sum(dc_backward[:self.hidden_size] * self.c_backward[t])\n",
    "            dgates_backward[2][:self.hidden_size] = np.sum(dh_backward[:self.hidden_size] * tanh(self.c_backward[t + 1]) * sigmoid_derivative(dgates_backward[2]))\n",
    "            \n",
    "            x_transpose = np.array([x[t]])\n",
    "            dgates_transpose_forward_0 = np.array([dgates_forward[0]])\n",
    "            dgates_transpose_forward_1 = np.array([dgates_forward[1]])\n",
    "            dgates_transpose_forward_2 = np.array([dgates_forward[2]])\n",
    "            dgates_transpose_forward_3 = np.array([dgates_forward[3]])\n",
    "\n",
    "            dgates_transpose_backward_0 = np.array([dgates_backward[0]])\n",
    "            dgates_transpose_backward_1 = np.array([dgates_backward[1]])\n",
    "            dgates_transpose_backward_2 = np.array([dgates_backward[2]])\n",
    "            dgates_transpose_backward_3 = np.array([dgates_backward[3]])\n",
    "            \n",
    "            dW_i += dgates_transpose_forward_0.T @ x_transpose + dgates_transpose_backward_0.T @ x_transpose\n",
    "            dU_i += dgates_forward[0] @ self.h_forward[t].T + dgates_backward[0] @ self.h_backward[t].T\n",
    "            db_i += dgates_transpose_forward_0.T + dgates_transpose_backward_0.T\n",
    "            \n",
    "            dW_f += dgates_transpose_forward_1.T @ x_transpose + dgates_transpose_backward_1.T @ x_transpose # Error disini\n",
    "            dU_f += dgates_forward[1] @ self.h_forward[t].T + dgates_backward[1] @ self.h_backward[t].T\n",
    "            db_f += dgates_transpose_forward_1.T + dgates_transpose_backward_1.T\n",
    "            \n",
    "            dW_o += dgates_transpose_forward_2.T @ x_transpose + dgates_transpose_backward_2.T @ x_transpose\n",
    "            dU_o += dgates_forward[2] @ self.h_forward[t].T + dgates_backward[2] @ self.h_backward[t].T\n",
    "            db_o += dgates_transpose_forward_2.T + dgates_transpose_backward_2.T\n",
    "            \n",
    "            dW_c += dgates_transpose_forward_3.T @ x_transpose + dgates_transpose_backward_3.T @ x_transpose\n",
    "            dU_c += dgates_forward[3] @ self.h_forward[t].T + dgates_backward[3] @ self.h_backward[t].T\n",
    "            db_c += dgates_transpose_forward_3.T + dgates_transpose_backward_3.T\n",
    "            \n",
    "            dh_next = self.U_i.T @ dgates_forward[0] + self.U_f.T @ dgates_forward[1] + self.U_o.T @ dgates_forward[2] + self.U_c.T @ dgates_forward[3]\n",
    "            dc_next = dc_forward * self.gates_forward[t, 1]\n",
    "        \n",
    "        for param in [dW_i, dU_i, db_i, dW_f, dU_f, db_f, dW_o, dU_o, db_o, dW_c, dU_c, db_c, dW_y, db_y]:\n",
    "            np.clip(param, -1, 1, out=param)\n",
    "        \n",
    "        self.W_i -= learning_rate * dW_i\n",
    "        self.U_i -= learning_rate * dU_i\n",
    "        self.b_i -= learning_rate * db_i\n",
    "        \n",
    "        self.W_f -= learning_rate * dW_f\n",
    "        self.U_f -= learning_rate * dU_f\n",
    "        self.b_f -= learning_rate * db_f\n",
    "        \n",
    "        self.W_o -= learning_rate * dW_o\n",
    "        self.U_o -= learning_rate * dU_o\n",
    "        self.b_o -= learning_rate * db_o\n",
    "        \n",
    "        self.W_c -= learning_rate * dW_c\n",
    "        self.U_c -= learning_rate * dU_c\n",
    "        self.b_c -= learning_rate * db_c\n",
    "        \n",
    "        self.W_y -= learning_rate * dW_y\n",
    "        self.b_y -= learning_rate * db_y\n",
    "    \n",
    "    def train(self, x, y, epochs, learning_rate):\n",
    "        for epoch in range(epochs):\n",
    "            outputs = self.forward_backward(x)\n",
    "            self.backPropagation(x, y, learning_rate)\n",
    "            # print(f'Hasil Prediksi {outputs}')\n",
    "            outputs = np.clip(outputs, 1e-15, None)\n",
    "            loss = -np.mean(np.log(outputs[np.arange(len(y)), y]))\n",
    "            \n",
    "            print(f'Epoch {epoch + 1}/{epochs}, Loss: {loss:.4f}')\n",
    "    \n",
    "    def predict(self, x):\n",
    "        T = len(x)\n",
    "        h_forward = np.zeros((T + 1, self.hidden_size))\n",
    "        c_forward = np.zeros((T + 1, self.hidden_size))\n",
    "        h_backward = np.zeros((T + 1, self.hidden_size))\n",
    "        c_backward = np.zeros((T + 1, self.hidden_size))\n",
    "        outputs = np.zeros((T, self.output_size))\n",
    "        \n",
    "        for t in range(T):\n",
    "            # Forward\n",
    "            gates_forward = np.concatenate([\n",
    "                sigmoid(self.W_i @ x[t] + self.U_i @ h_forward[t] + self.b_i),\n",
    "                sigmoid(self.W_f @ x[t] + self.U_f @ h_forward[t] + self.b_f),\n",
    "                sigmoid(self.W_o @ x[t] + self.U_o @ h_forward[t] + self.b_o),\n",
    "                tanh(self.W_c @ x[t] + self.U_c @ h_forward[t] + self.b_c)\n",
    "            ])\n",
    "                \n",
    "            c_forward[t + 1] = gates_forward[1] * c_forward[t] + gates_forward[0] * gates_forward[3]\n",
    "            h_forward[t + 1] = gates_forward[2] * tanh(c_forward[t + 1])\n",
    "                \n",
    "            # Backward\n",
    "            t_backward = T - t - 1\n",
    "            gates_backward = np.concatenate([\n",
    "                sigmoid(self.W_i @ x[t_backward] + self.U_i @ h_backward[t_backward] + self.b_i),\n",
    "                sigmoid(self.W_f @ x[t_backward] + self.U_f @ h_backward[t_backward] + self.b_f),\n",
    "                sigmoid(self.W_o @ x[t_backward] + self.U_o @ h_backward[t_backward] + self.b_o),\n",
    "                tanh(self.W_c @ x[t_backward] + self.U_c @ h_backward[t_backward] + self.b_c)\n",
    "            ])\n",
    "                \n",
    "            c_backward[t_backward + 1] = gates_backward[1] * c_backward[t_backward] + gates_backward[0] * gates_backward[3]\n",
    "            h_backward[t_backward + 1] = gates_backward[2] * tanh(c_backward[t_backward + 1])\n",
    "\n",
    "            merged_output = np.concatenate([h_forward[t + 1], h_backward[t_backward + 1]])\n",
    "            sigmoids = sigmoid(self.W_y @ merged_output + self.b_y)\n",
    "            outputs[t] = sigmoids[0]\n",
    "        \n",
    "        return np.argmax(outputs, axis=1)\n",
    "    \n",
    "    def save_model(self, model, filename):\n",
    "        with open(filename, 'wb') as f:\n",
    "            pkl.dump(model, f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Training & Testing Model</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>\n",
    "Percobaan 1 :\n",
    "embedding size = 50\n",
    "max_len = 25\n",
    "hidden_size = 1\n",
    "\n",
    "Percobaan 2 :\n",
    "embedding size = 25\n",
    "max_len = 25\n",
    "hidden_size = 2\n",
    "\n",
    "Percobaan 3:\n",
    "Menambahkan value dari fitur sentiment pada data Training\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, ..., 1, 1, 1], dtype=int64)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_oversampled"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Bi-LSTM</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = X_oversampled\n",
    "x_train = np.array(x_train)\n",
    "# label = final_stack_label.astype('int')\n",
    "y_trains = y_oversampled\n",
    "\n",
    "# Inisialisasi model Bi-LSTM\n",
    "input_size = x_train.shape[1]\n",
    "hidden_size = 7\n",
    "output_size = 2\n",
    "bilstm = BiLSTM(input_size, hidden_size, output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_trains.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150, Loss: 34.5388\n",
      "Epoch 2/150, Loss: 34.5388\n",
      "Epoch 3/150, Loss: 34.5388\n",
      "Epoch 4/150, Loss: 34.5388\n",
      "Epoch 5/150, Loss: 34.5388\n",
      "Epoch 6/150, Loss: 34.5388\n",
      "Epoch 7/150, Loss: 34.5388\n",
      "Epoch 8/150, Loss: 34.5388\n",
      "Epoch 9/150, Loss: 34.5388\n",
      "Epoch 10/150, Loss: 34.5388\n",
      "Epoch 11/150, Loss: 34.5388\n",
      "Epoch 12/150, Loss: 34.5388\n",
      "Epoch 13/150, Loss: 34.5388\n",
      "Epoch 14/150, Loss: 34.5388\n",
      "Epoch 15/150, Loss: 34.5388\n",
      "Epoch 16/150, Loss: 34.5388\n",
      "Epoch 17/150, Loss: 34.5388\n",
      "Epoch 18/150, Loss: 34.5388\n",
      "Epoch 19/150, Loss: 34.5388\n",
      "Epoch 20/150, Loss: 34.5388\n",
      "Epoch 21/150, Loss: 34.5388\n",
      "Epoch 22/150, Loss: 34.5388\n",
      "Epoch 23/150, Loss: 34.5388\n",
      "Epoch 24/150, Loss: 34.5388\n",
      "Epoch 25/150, Loss: 34.5388\n",
      "Epoch 26/150, Loss: 34.5388\n",
      "Epoch 27/150, Loss: 34.5388\n",
      "Epoch 28/150, Loss: 34.5388\n",
      "Epoch 29/150, Loss: 34.5388\n",
      "Epoch 30/150, Loss: 34.5388\n",
      "Epoch 31/150, Loss: 34.5388\n",
      "Epoch 32/150, Loss: 34.5388\n",
      "Epoch 33/150, Loss: 34.5388\n",
      "Epoch 34/150, Loss: 34.5388\n",
      "Epoch 35/150, Loss: 34.5388\n",
      "Epoch 36/150, Loss: 34.5388\n",
      "Epoch 37/150, Loss: 34.5388\n",
      "Epoch 38/150, Loss: 34.5388\n",
      "Epoch 39/150, Loss: 34.5388\n",
      "Epoch 40/150, Loss: 34.5388\n",
      "Epoch 41/150, Loss: 34.5388\n",
      "Epoch 42/150, Loss: 34.5388\n",
      "Epoch 43/150, Loss: 34.5388\n",
      "Epoch 44/150, Loss: 34.5388\n",
      "Epoch 45/150, Loss: 34.5388\n",
      "Epoch 46/150, Loss: 34.5388\n",
      "Epoch 47/150, Loss: 34.5388\n",
      "Epoch 48/150, Loss: 34.5388\n",
      "Epoch 49/150, Loss: 34.5388\n",
      "Epoch 50/150, Loss: 34.5388\n",
      "Epoch 51/150, Loss: 34.5388\n",
      "Epoch 52/150, Loss: 34.5388\n",
      "Epoch 53/150, Loss: 34.5388\n",
      "Epoch 54/150, Loss: 34.5388\n",
      "Epoch 55/150, Loss: 34.5388\n",
      "Epoch 56/150, Loss: 34.5388\n",
      "Epoch 57/150, Loss: 34.5388\n",
      "Epoch 58/150, Loss: 34.5388\n",
      "Epoch 59/150, Loss: 34.5388\n",
      "Epoch 60/150, Loss: 34.5388\n",
      "Epoch 61/150, Loss: 34.5388\n",
      "Epoch 62/150, Loss: 34.5388\n",
      "Epoch 63/150, Loss: 34.5388\n",
      "Epoch 64/150, Loss: 34.5388\n",
      "Epoch 65/150, Loss: 34.5388\n",
      "Epoch 66/150, Loss: 34.5388\n",
      "Epoch 67/150, Loss: 34.5388\n",
      "Epoch 68/150, Loss: 34.5388\n",
      "Epoch 69/150, Loss: 34.5388\n",
      "Epoch 70/150, Loss: 34.5388\n",
      "Epoch 71/150, Loss: 34.5388\n",
      "Epoch 72/150, Loss: 34.5388\n",
      "Epoch 73/150, Loss: 34.5388\n",
      "Epoch 74/150, Loss: 34.5388\n",
      "Epoch 75/150, Loss: 34.5388\n",
      "Epoch 76/150, Loss: 34.5388\n",
      "Epoch 77/150, Loss: 34.5388\n",
      "Epoch 78/150, Loss: 34.5388\n",
      "Epoch 79/150, Loss: 34.5388\n",
      "Epoch 80/150, Loss: 34.5388\n",
      "Epoch 81/150, Loss: 34.5388\n",
      "Epoch 82/150, Loss: 34.5388\n",
      "Epoch 83/150, Loss: 34.5388\n",
      "Epoch 84/150, Loss: 34.5388\n",
      "Epoch 85/150, Loss: 34.5388\n",
      "Epoch 86/150, Loss: 34.5388\n",
      "Epoch 87/150, Loss: 34.5388\n",
      "Epoch 88/150, Loss: 34.5388\n",
      "Epoch 89/150, Loss: 34.5388\n",
      "Epoch 90/150, Loss: 34.5388\n",
      "Epoch 91/150, Loss: 34.5388\n",
      "Epoch 92/150, Loss: 34.5388\n",
      "Epoch 93/150, Loss: 34.5388\n",
      "Epoch 94/150, Loss: 34.5388\n",
      "Epoch 95/150, Loss: 34.5388\n",
      "Epoch 96/150, Loss: 34.5388\n",
      "Epoch 97/150, Loss: 34.5388\n",
      "Epoch 98/150, Loss: 34.5388\n",
      "Epoch 99/150, Loss: 34.5388\n",
      "Epoch 100/150, Loss: 34.5388\n",
      "Epoch 101/150, Loss: 34.5388\n",
      "Epoch 102/150, Loss: 34.5388\n",
      "Epoch 103/150, Loss: 34.5388\n",
      "Epoch 104/150, Loss: 34.5388\n",
      "Epoch 105/150, Loss: 34.5388\n",
      "Epoch 106/150, Loss: 34.5388\n",
      "Epoch 107/150, Loss: 34.5388\n",
      "Epoch 108/150, Loss: 34.5388\n",
      "Epoch 109/150, Loss: 34.5388\n",
      "Epoch 110/150, Loss: 34.5388\n",
      "Epoch 111/150, Loss: 34.5388\n",
      "Epoch 112/150, Loss: 34.5388\n",
      "Epoch 113/150, Loss: 34.5388\n",
      "Epoch 114/150, Loss: 34.5388\n",
      "Epoch 115/150, Loss: 34.5388\n",
      "Epoch 116/150, Loss: 34.5388\n",
      "Epoch 117/150, Loss: 34.5388\n",
      "Epoch 118/150, Loss: 34.5388\n",
      "Epoch 119/150, Loss: 34.5388\n",
      "Epoch 120/150, Loss: 34.5388\n",
      "Epoch 121/150, Loss: 34.5388\n",
      "Epoch 122/150, Loss: 34.5388\n",
      "Epoch 123/150, Loss: 34.5388\n",
      "Epoch 124/150, Loss: 34.5388\n",
      "Epoch 125/150, Loss: 34.5388\n",
      "Epoch 126/150, Loss: 34.5388\n",
      "Epoch 127/150, Loss: 34.5388\n",
      "Epoch 128/150, Loss: 34.5388\n",
      "Epoch 129/150, Loss: 34.5388\n",
      "Epoch 130/150, Loss: 34.5388\n",
      "Epoch 131/150, Loss: 34.5388\n",
      "Epoch 132/150, Loss: 34.5388\n",
      "Epoch 133/150, Loss: 34.5388\n",
      "Epoch 134/150, Loss: 34.5388\n",
      "Epoch 135/150, Loss: 34.5388\n",
      "Epoch 136/150, Loss: 34.5388\n",
      "Epoch 137/150, Loss: 34.5388\n",
      "Epoch 138/150, Loss: 34.5388\n",
      "Epoch 139/150, Loss: 34.5388\n",
      "Epoch 140/150, Loss: 34.5388\n",
      "Epoch 141/150, Loss: 34.5388\n",
      "Epoch 142/150, Loss: 34.5388\n",
      "Epoch 143/150, Loss: 34.5388\n",
      "Epoch 144/150, Loss: 34.5388\n",
      "Epoch 145/150, Loss: 34.5388\n",
      "Epoch 146/150, Loss: 34.5388\n",
      "Epoch 147/150, Loss: 34.5388\n",
      "Epoch 148/150, Loss: 34.5388\n",
      "Epoch 149/150, Loss: 34.5388\n",
      "Epoch 150/150, Loss: 34.5388\n"
     ]
    }
   ],
   "source": [
    "# Train ML Model\n",
    "epochs = 150\n",
    "learning_rate = 0.001\n",
    "bilstm.train(x_train, y_trains, epochs, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(439, 3900)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_embedding_r.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.04268292,  0.14066966,  0.20659287, ...,  0.02416898,\n",
       "        0.06397357, -0.03179826])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_embedding_r[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1,\n",
       "       0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1,\n",
       "       1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1,\n",
       "       0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1,\n",
       "       0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1,\n",
       "       0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0,\n",
       "       1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0,\n",
       "       1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1,\n",
       "       1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0,\n",
       "       0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1,\n",
       "       1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0,\n",
       "       0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0,\n",
       "       0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1,\n",
       "       0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,\n",
       "       0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0,\n",
       "       0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0,\n",
       "       1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1,\n",
       "       1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0,\n",
       "       1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict\n",
    "output_predict = []\n",
    "for i in range(len(X_test_embedding_r)):\n",
    "    output_predict.append(bilstm.predict(np.array([X_test_embedding_r[i]])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([1], dtype=int64),\n",
       " array([1], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([1], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([1], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([1], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([1], dtype=int64),\n",
       " array([1], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([1], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([1], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([1], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([1], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([1], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([1], dtype=int64),\n",
       " array([1], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([1], dtype=int64),\n",
       " array([1], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([1], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([1], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([1], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([1], dtype=int64),\n",
       " array([1], dtype=int64),\n",
       " array([1], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([1], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([1], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([1], dtype=int64),\n",
       " array([1], dtype=int64),\n",
       " array([1], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([1], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([1], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([1], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([1], dtype=int64),\n",
       " array([1], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([1], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([1], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([1], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([1], dtype=int64),\n",
       " array([1], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([1], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([1], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([1], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([1], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([1], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([1], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([1], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([1], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([1], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([1], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([1], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([1], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([1], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([1], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([1], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([1], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([1], dtype=int64),\n",
       " array([1], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([1], dtype=int64),\n",
       " array([1], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([1], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([1], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([1], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([1], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([1], dtype=int64),\n",
       " array([1], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64)]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result = pd.DataFrame.from_records(output_predict, columns=['Predict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 439 entries, 0 to 438\n",
      "Data columns (total 1 columns):\n",
      " #   Column   Non-Null Count  Dtype\n",
      "---  ------   --------------  -----\n",
      " 0   Predict  439 non-null    int64\n",
      "dtypes: int64(1)\n",
      "memory usage: 3.6 KB\n"
     ]
    }
   ],
   "source": [
    "df_result.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_actual = []\n",
    "\n",
    "for i in y_test:\n",
    "    output_actual.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_actual = pd.DataFrame(output_actual, columns=['Actual'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_result = df_result.join(df_actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predict</th>\n",
       "      <th>Actual</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Predict  Actual\n",
       "0         0       0\n",
       "1         0       1\n",
       "2         0       0\n",
       "3         0       1\n",
       "4         0       1\n",
       "5         0       1\n",
       "6         0       1\n",
       "7         0       0\n",
       "8         0       1\n",
       "9         0       0\n",
       "10        0       0"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final_result.head(11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(df_final_result)):\n",
    "  if (df_final_result.iloc[i]['Predict'] == 1 and df_final_result.iloc[i]['Actual'] == 1):\n",
    "    df_final_result.loc[i, 'Category'] = 'TP'\n",
    "  elif (df_final_result.iloc[i]['Predict'] == 0 and df_final_result.iloc[i]['Actual'] == 0):\n",
    "    df_final_result.loc[i, 'Category'] = 'TN'\n",
    "  elif (df_final_result.iloc[i]['Predict'] == 1 and df_final_result.iloc[i]['Actual'] == 0):\n",
    "    df_final_result.loc[i, 'Category'] = 'FP'\n",
    "  elif (df_final_result.iloc[i]['Predict'] == 0 and df_final_result.iloc[i]['Actual'] == 1):\n",
    "    df_final_result.loc[i, 'Category'] = 'FN'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "TP = len(df_final_result[df_final_result['Category']=='TP'])\n",
    "TN = len(df_final_result[df_final_result['Category']=='TN'])\n",
    "FP = len(df_final_result[df_final_result['Category']=='FP'])\n",
    "FN = len(df_final_result[df_final_result['Category']=='FN'])\n",
    "\n",
    "accuracy = round((TP + TN) / (TP + TN + FP + FN), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 56.99999999999999%\n"
     ]
    }
   ],
   "source": [
    "print(f'Accuracy: {accuracy * 100}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Testing Result</h3>\n",
    "<p>July 20 : 50%</p>\n",
    "<p>July 21 : 53%</p>\n",
    "<p>July 22 : 54%</p>\n",
    "<p>July 25 : 61%</p>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Sample Datasets BAB 3</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bab3 = pd.read_excel('Misc/Sample_Datasets_Bab3_Ready.xlsx')\n",
    "df_bab3.info()\n",
    "df_bab3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove unused column/feature\n",
    "df_bab3 = df_bab3.drop(columns='Unnamed: 0')\n",
    "df_bab3 = df_bab3.drop(columns='Text')\n",
    "df_bab3 = df_bab3.drop(columns='emoji')\n",
    "df_bab3 = df_bab3.drop(columns='Tokenized')\n",
    "df_bab3 = df_bab3.drop(columns='final_text')\n",
    "df_bab3 = df_bab3.drop(columns='Pos_Word')\n",
    "df_bab3 = df_bab3.drop(columns='Neg_Word')\n",
    "df_bab3 = df_bab3.drop(columns='Total_Word')\n",
    "df_bab3 = df_bab3.drop(columns='Pos_Ratio')\n",
    "df_bab3 = df_bab3.drop(columns='Neg_Ratio')\n",
    "df_bab3 = df_bab3.drop(columns='Tokenize_Emoji')\n",
    "df_bab3 = df_bab3.drop(columns='Pos_Emoji')\n",
    "df_bab3 = df_bab3.drop(columns='Neg_Emoji')\n",
    "df_bab3 = df_bab3.drop(columns='Sentimen_Emoji')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bab3['Sarcasm'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bab3['text_emoji'][0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Word Embedding</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Membangun model GloVe\n",
    "glove_model = GloVe(df_bab3['text_emoji'])\n",
    "\n",
    "# Membangun vocabulary\n",
    "glove_model.build_vocab_information()\n",
    "\n",
    "# Membangun co-occurrence matrix\n",
    "glove_model.build_co_matrix()\n",
    "\n",
    "# Melatih model GloVe\n",
    "glove_model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word, id in glove_model.word2count.items():\n",
    "    print(f'{word}: {id}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# glove_model.generate_coomatrix_image()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_model.get_all_embeddings()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>SMOTE</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bab3['Sentimen_Text'] = df_bab3['Sentimen_Text'].replace('Positif', 1)\n",
    "df_bab3['Sentimen_Text'] = df_bab3['Sentimen_Text'].replace('Negatif', 0)\n",
    "\n",
    "df_bab3['Sarcasm'] = df_bab3['Sarcasm'].replace('Positif', 1)\n",
    "df_bab3['Sarcasm'] = df_bab3['Sarcasm'].replace('Negatif', 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cek class minoritas\n",
    "df_bab3['Sarcasm'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 50\n",
    "len_voc = 40000\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df_train, df_test = train_test_split(df_bab3, test_size=0.3, random_state=42, shuffle=False)\n",
    "\n",
    "df_train_min = df_train[df_train['Sarcasm'] == 0]\n",
    "df_train_maj = df_train[df_train['Sarcasm'] == 1]\n",
    "\n",
    "df_test_min = df_test[df_test['Sarcasm'] == 0]\n",
    "df_test_maj = df_test[df_test['Sarcasm'] == 1]\n",
    "\n",
    "df_ready_min = pd.concat([df_train_min, df_test_min])\n",
    "df_ready_maj = pd.concat([df_train_maj, df_test_maj])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer\n",
    "tokenizer_min = make_tokenizer(df_ready_min['text_emoji'], len_voc)\n",
    "tokenizer_maj = make_tokenizer(df_ready_maj['text_emoji'], len_voc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sequence\n",
    "X_train_min = tokenizer_min.texts_to_sequences(df_train_min['text_emoji'])\n",
    "X_test_min = tokenizer_min.texts_to_sequences(df_test_min['text_emoji'])\n",
    "X_train_major = tokenizer_maj.texts_to_sequences(df_train_maj['text_emoji'])\n",
    "X_test_major = tokenizer_maj.texts_to_sequences(df_test_maj['text_emoji'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Padding\n",
    "from keras.utils import pad_sequences\n",
    "\n",
    "X_train_min = pad_sequences(X_train_min, maxlen=max_len, padding='post', truncating='post')\n",
    "X_test_min = pad_sequences(X_test_min, maxlen=max_len, padding='post', truncating='post')\n",
    "X_train_major = pad_sequences(X_train_major, maxlen=max_len, padding='post', truncating='post')\n",
    "X_test_major = pad_sequences(X_test_major, maxlen=max_len, padding='post', truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stacking Label Minority\n",
    "y_train_min = df_train_min['Sarcasm'].values\n",
    "y_test_min = df_test_min['Sarcasm'].values\n",
    "\n",
    "stack_label_min = np.hstack((y_train_min, y_test_min))\n",
    "\n",
    "stack_label_min.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stacking Label Majority\n",
    "y_train_maj = df_train_maj['Sarcasm'].values\n",
    "y_test_maj = df_test_maj['Sarcasm'].values\n",
    "\n",
    "stack_label_maj = np.hstack((y_train_maj, y_test_maj))\n",
    "\n",
    "stack_label_maj.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stacking All Data Minority & Majority\n",
    "embed_mat_min = make_embedding_matrix(glove, tokenizer_min, len_voc)\n",
    "embed_mat_maj = make_embedding_matrix(glove, tokenizer_maj, len_voc)\n",
    "\n",
    "X_train_emb_minority = embed_mat_min[X_train_min]\n",
    "X_test_emb_minority = embed_mat_min[X_test_min]\n",
    "\n",
    "X_train_emb_majority = embed_mat_maj[X_train_major]\n",
    "X_test_emb_majority = embed_mat_maj[X_test_major]\n",
    "\n",
    "train_size_min, max_len_min, embed_size_min = X_train_emb_minority.shape\n",
    "X_train_emb_r_min = X_train_emb_minority.reshape(train_size_min, max_len*embed_size_min)\n",
    "\n",
    "test_size_min, max_len_min, embed_size_min = X_test_emb_minority.shape\n",
    "X_test_emb_r_min = X_test_emb_minority.reshape(test_size_min, max_len*embed_size_min)\n",
    "\n",
    "train_size_maj, max_len_maj, embed_size_maj = X_train_emb_majority.shape\n",
    "X_train_emb_r_maj = X_train_emb_majority.reshape(train_size_maj, max_len*embed_size_maj)\n",
    "\n",
    "test_size_maj, max_len_maj, embed_size_maj = X_test_emb_majority.shape\n",
    "X_test_emb_r_maj = X_test_emb_majority.reshape(test_size_maj, max_len*embed_size_maj)\n",
    "\n",
    "stack_minority = np.vstack((X_train_emb_r_min, X_test_emb_r_min))\n",
    "\n",
    "stack_majority = np.vstack((X_train_emb_r_maj, X_test_emb_r_maj))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_emb_minority[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Shape stack minority: {stack_minority.shape}')\n",
    "print(f'Shape stack majority{stack_majority.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running SMOTE\n",
    "total_size = train_size_maj + train_size_min + test_size_maj + test_size_min\n",
    "\n",
    "X_minority = stack_minority\n",
    "y_minority = stack_label_min\n",
    "X_majority = stack_majority\n",
    "y_majority = stack_label_maj\n",
    "\n",
    "X_oversampled, y_oversampled = smote(X_minority, X_majority, y_minority, k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_stack = np.vstack((stack_majority, X_oversampled))\n",
    "\n",
    "len(final_stack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_stack = np.delete(final_stack, np.s_[10::], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_stack_label = np.hstack((stack_label_maj, y_oversampled))\n",
    "\n",
    "final_stack_label = np.delete(final_stack_label, np.s_[10::], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Shape : {final_stack.shape}\\n')\n",
    "print(final_stack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = ['Sebelum', 'Sesudah']\n",
    "y = [len(stack_majority) + len(stack_minority), len(final_stack)]\n",
    "\n",
    "\n",
    "plt.bar(x, y, color=[(0.38, 0.62, 0.79), (0.98, 0.64, 0.33)], edgecolor='black', linewidth=1.5)\n",
    "\n",
    "for i in range(len(x)):\n",
    "    plt.text(x[i], y[i], str(y[i]), ha='center', va='bottom')\n",
    "\n",
    "plt.ylabel('Jumlah Sample')\n",
    "plt.title('Perbandingan Data Setelah Diimplementasi SMOTE')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = ['0', '1']\n",
    "y = [df_bab3['Sarcasm'].value_counts()[0], df_bab3['Sarcasm'].value_counts()[1]]\n",
    "\n",
    "\n",
    "plt.bar(x, y, color=[(0.38, 0.62, 0.79), (0.98, 0.64, 0.33)], edgecolor='black', linewidth=1.5)\n",
    "\n",
    "for i in range(len(x)):\n",
    "    plt.text(x[i], y[i], str(y[i]), ha='center', va='bottom')\n",
    "\n",
    "plt.ylabel('Jumlah Sample')\n",
    "plt.title('Jumlah Sample Masing-Masing Kelas Sebelum Di Implementasi SMOTE')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Menghitung Jumlah Akhir Sample pada Class 0 dan 1\n",
    "unique, counts = np.unique(final_stack_label, return_counts=True)\n",
    "result = np.asarray((unique, counts)).T.astype(int)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = ['0', '1']\n",
    "y = [result[0][1], result[1][1]]\n",
    "\n",
    "\n",
    "plt.bar(x, y, color=[(0.38, 0.62, 0.79), (0.98, 0.64, 0.33)], edgecolor='black', linewidth=1.5)\n",
    "\n",
    "for i in range(len(x)):\n",
    "    plt.text(x[i], y[i], str(y[i]), ha='center', va='bottom')\n",
    "\n",
    "plt.ylabel('Jumlah Sample')\n",
    "plt.title('Jumlah Sample Masing-Masing Kelas Setelah Di Implementasi SMOTE')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Bi-LSTM</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = final_stack\n",
    "x_train = np.array(x_train)\n",
    "label = final_stack_label.astype('int')\n",
    "y_train = label\n",
    "\n",
    "# Inisialisasi model Bi-LSTM\n",
    "input_size = x_train.shape[1]\n",
    "hidden_size = 8\n",
    "output_size = 2\n",
    "bilstm = BiLSTM(input_size, hidden_size, output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train ML Model\n",
    "epochs = 100\n",
    "learning_rate = 0.01\n",
    "bilstm.train(x_train, y_train, epochs, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train[9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict ML Model\n",
    "print(f'Predict Data 1 : {bilstm.predict(np.array([x_train[0]]))}\\n')\n",
    "print(f'Predict Data 2 : {bilstm.predict(np.array([x_train[1]]))}\\n')\n",
    "print(f'Predict Data 3 : {bilstm.predict(np.array([x_train[2]]))}\\n')\n",
    "print(f'Predict Data 4 : {bilstm.predict(np.array([x_train[3]]))}\\n')\n",
    "print(f'Predict Data 5 : {bilstm.predict(np.array([x_train[4]]))}\\n')\n",
    "print(f'Predict Data 6 : {bilstm.predict(np.array([x_train[5]]))}\\n')\n",
    "print(f'Predict Data 7 : {bilstm.predict(np.array([x_train[6]]))}\\n')\n",
    "print(f'Predict Data 8 : {bilstm.predict(np.array([x_train[7]]))}\\n')\n",
    "print(f'Predict Data 9 : {bilstm.predict(np.array([x_train[8]]))}\\n')\n",
    "print(f'Predict Data 10 : {bilstm.predict(np.array([x_train[9]]))}\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
